---
title: "CompulsoryExercise_1"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import neccessary packages

```{r libraries, message=FALSE, include=TRUE, eval=TRUE}
library(ggplot2)
library(caret)
library(MASS)
library(class)
library(pROC)
```


# Task 1

## D)

Load the data.
```{r data, include=TRUE, message=FALSE, eval=TRUE}
id <- "1X_8OKcoYbng1XvYFDirxjEWr7LtpNr1m"  # google file ID
values <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
X = values$X
x0 = values$x0
beta = values$beta
sigma = values$sigma
```


Display squared bias.
```{r squared bias, include=TRUE, message=FALSE, eval=TRUE, tidy=TRUE}
bias = function(lambda, X, x0, beta) {
  p = ncol(X)
  value = (t(x0) %*% solve((t(X) %*% X + lambda * diag(p))) %*% t(X) %*% X %*% beta - t(x0) %*% beta ) ^ 2
  return(value)
}
lambdas = seq(0, 2, length.out = 500)
BIAS = rep(NA, length(lambdas))
for (i in 1:length(lambdas)) BIAS[i] = bias(lambdas[i], X, x0, beta)
dfBias = data.frame(lambdas = lambdas, bias = BIAS)
ggplot(dfBias, aes(x = lambdas, y = bias)) + geom_line(color = "red") + xlab(expression(lambda)) +
  ylab(expression(bias^2))
```

### TODO: Fill in value (squared bias) and comment on what we see.

## E)

```{r variance, include=TRUE, message=FALSE, eval=TRUE, tidy=TRUE}
variance = function(lambda, X, x0, sigma) {
  p = ncol(X)
  inv = solve(t(X) %*% X + lambda * diag(p))
  value = sigma * sigma * t(x0) %*% solve(t(X) %*% X + lambda * diag(p)) %*% t(X) %*% X %*% solve(t(X) %*% X + lambda * diag(p)) %*% x0
  return(value)
}
lambdas = seq(0, 2, length.out = 500)
VAR = rep(NA, length(lambdas))
for (i in 1:length(lambdas)) VAR[i] = variance(lambdas[i], X, x0, sigma)
dfVar = data.frame(lambdas = lambdas, var = VAR)
ggplot(dfVar, aes(x = lambdas, y = var)) + geom_line(color = "green4") + xlab(expression(lambda)) +
  ylab("variance")
```
### TODO: Fill in value and comment on what we see.

## F)

```{r expected mse, include=TRUE, message=FALSE, eval=TRUE, tidy=TRUE}

exp_mse = BIAS + VAR + sigma * sigma
lambdas[which.min(exp_mse)]

dfAll = data.frame(lambda = lambdas, bias = BIAS, var = VAR, exp_mse = exp_mse)
ggplot(dfAll) + geom_line(aes(x = lambda, y = exp_mse), color = "blue") +
  geom_line(aes(x = lambda, y = bias), color = "red") +
  geom_line(aes(x = lambda, y = var), color = "green4") +
  xlab(expression(lambda)) +
  ylab(expression(E(MSE))
)
```

### TODO: Fill in exp_mse and find value of lambda that minimizes mse.
### TODO: Comment on what we see.
### TODO: When generating the pdf, change to eval=True


# Problem 2

```
# read file
id <- "1yYlEl5gYY3BEtJ4d7KWaFGIOEweJIn__" # google file ID
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
summary(d.corona)

# Number of deceased and non-deceased
deceased <- nrow(filter(d.corona, d.corona$deceased == "1"))
non_deceased <- nrow(filter(d.corona, d.corona$deceased == "0"))

# Number of males and females for each country
number_of_males_and_females <- d.corona %>%
  group_by(country, sex) %>%
  summarise(amount = n()) %>%
  arrange(desc(amount))


# The number of deceased and non-deceased for each sex
number_of_deceased_and_non_deceased <- d.corona %>%
  group_by(deceased, sex) %>%
  summarise(amount = n()) %>%
  arrange(desc(amount)) 

# The number of deceased and non-deceased in France, separate for each sex.
number_of_deceased_and_non_deceased_in_France_for_each_sex <- filter(d.corona, d.corona$country == "France") %>%
  group_by(deceased, sex) %>%
  summarise(amount = n()) %>%
  arrange(desc(amount)) 


#Linear regression

train_ID = sample(1:nrow(d.corona), nrow(d.corona)/(3/2))
train_data = d.corona[train_ID, ]
test_data = d.corona[-train_ID, ]

age_vs_deceased <- d.corona %>%
  group_by(age, deceased) %>%
  summarise(amount = n()) %>%
  arrange(desc(amount))

age_country_deceased <- d.corona %>%
  group_by(age, deceased, country) %>%
  summarise(amount = n()) %>%
  arrange(desc(amount)) 

scatter_plot <- scatter.smooth(x=age_vs_deceased$age, y=age_vs_deceased$amount, main="Age ~ Deceased")
cor(d.corona$age, d.corona$deceased)

model <- glm(deceased ~ age + country + sex, family = binomial, data=train_data)
anova(model)
summary(model)
coefficients(model)

test <- filter( d.corona, d.corona$country == "Korea" & d.corona$age == "75" & d.corona$sex == "male")

create_confusion_matrix <- function(pred, target) {
  confMat <- table(pred, target)
  colnames(confMat) <- c("PRED FALSE", "PRED TRUE")
  row.names(confMat) <- c("TARGET FALSE", "TARGET TRUE")
  return(confMat)
}


# 1. What is the probability to die of covid for a male age 75 living in Korea?
data_point <- data.frame("age" = 75, "sex" = "male","country" = "Korea")
predict(model, newdata = data_point, type = "response")
# There are a 10.01 % chance of a 75 year-old korean male dying of covid. 

# 2. Is there evidence that males have higher probability to die than females?
# Yes, the regression model shows that there are a higher probability for males to die than females.
# There is statistically significant evidence that males have a higher probability of dying of covid. 
  
# 3. Is there evidence that the country of residence has an influence on the probability to decease?
# Yes, there is evidence that the country of residence has an impact on the probability to decease. There is a big difference between in the probability of decease in Korea, Japan, and Indonesia. Indonesia has a much higher probability of decease than the rest. 
# There is evidence that for France and Indonesia, the country of residence does not impace the probability as much. While for Korea and Japan it has a bigger influence. This is due to the estimate of the dummy variables for France and Indonesia has a larger Pr(>|z|). 

# 4. Quantify how the odds to die changes when someone with otherwise identical covariates is 10 years older than another person.
summary(model)
odds <- exp(10*coefficients(model)[2])

# C.

# Is age a greater risk factor for males than for females?

# Is age a greater risk factor for the French population than for the Indonesian population?

# D.

lda_model <- lda(deceased ~ age + country + sex, data=d.corona)
lda_pred <- predict(lda_model, newdata=test_data[-1])
conf_matrix <- create_confusion_matrix(unlist(lda_pred[1]), test_data$deceased)
sum(conf_matrix)
null_rate <- (conf_matrix[2,1] + conf_matrix[2,2]) / sum(conf_matrix) 
# 1. The "null rate" for misclassification is 5.22%, because this is the proportion of deaths among all cases in the dataset. 
# False

# 2. LDA is not a very useful method for this dataset.
# False

# 3. LDA has a specificity of 1.
specificity <- conf_matrix[1,1] / (conf_matrix[1,1] + conf_matrix[1,2])
# False

# 4. QDA has a lower sensitivity to classify deceased compared to LDA.
qda_model <- qda(deceased ~ ., data=d.corona)
qda_pred <- predict(qda_model, newdata=test_data[-1])
qda_conf <- create_confusion_matrix(unlist(qda_pred[1]), test_data$deceased)
sens_lda <- conf_matrix[2,1] / (sum(conf_matrix[2,]))
sens_qda <- qda_conf[2,1] / (sum(qda_conf[2,]))
# False, they are equal
```

# Task 3


```{r data problem 3, include=TRUE, message=FALSE, eval=TRUE, tidy=TRUE}
# read file
id <- "1i1cQPeoLLC_FyAH0nnqCnnrSBpn05_hO"  # google file ID
diab <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))

t = MASS::Pima.tr2
train = diab$ctrain
test = diab$ctest
```

## A)

### i)
#### TODO: Show that...

### ii)
```{r logistic regression, include=TRUE, message=FALSE, eval=TRUE, tidy=TRUE}
logReg = glm(diabetes ~ ., data = train, family = "binomial")
summary(logReg)

create_confusion_matrix <- function(pred, target) {
  confMat <- table(pred, test$diabetes)
  colnames(confMat) <- c("PRED FALSE", "PRED TRUE")
  row.names(confMat) <- c("TARGET FALSE", "TARGET TRUE")
  return(confMat)
}

cutOff = 0.5
pred_log_reg <- predict(logReg, newdata = test[-1], type="response")
conf_mat = create_confusion_matrix(pred_log_reg>cutOff, test$diabetes)
conf_mat

print("The sensitivity is:")
conf_mat[2,2]/(conf_mat[2,1]+conf_mat[2,2])
print("The specificity is:")
conf_mat[1,1]/(conf_mat[1,2]+conf_mat[1,1])
```


## B)

### i)
#### TODO: Explain what the coefficients are.

### ii)
```{r linear-/quadretic discriminant analysis, include=TRUE, message=FALSE, eval=TRUE, tidy=TRUE}
lda_model <- lda(diabetes ~ ., data=train)
pred_lda <- predict(lda_model, newdata=test[-1])
conf_mat = create_confusion_matrix(unlist(pred_lda[1]), test$diabetes)
conf_mat
# With cutOff = 0.5: pred[1]
# Posterior probabilities: pred[2]

qda_model <- qda(diabetes ~ ., data=train)
pred_qda <- predict(qda_model, newdata=test[-1])
conf_mat = create_confusion_matrix(unlist(pred_qda[1]), test$diabetes)
conf_mat

```
#### TODO: Explain difference between the models.

## C)

### i)
#### TODO: Explain how a new observation is classified.

### ii)
#### TODO: Explain how to choose the tuning parameter.

### iii)
```{r knn, include=TRUE, message=FALSE, eval=TRUE, tidy=TRUE}
pred_knn <- knn(train = train[-1], test=test[-1], cl=unlist(train[1]), k=25, prob=TRUE)

convert <- function(pred, prob) {
  # Invert probabilities: the probabilites from knn(...) are the success probabilities
  # for the predicted class, thus P(y=2) = 1 - P(y=1) when we predicted 1.
  inv_prob = c()
  for(pos in 1:length(prob)) {
    inv_prob[pos] <- ifelse(as.numeric(pred[pos])==2, prob[pos], 1-prob[pos])
  }
  return(inv_prob)
}
prob_knn_adj <- convert(pred_knn, attributes(pred_knn)$prob)

conf_mat = create_confusion_matrix(unlist(pred_knn), test$diabetes)
conf_mat
print("The sensitivity is:")
conf_mat[2,2]/(conf_mat[2,1]+conf_mat[2,2])
print("The specificity is:")
conf_mat[1,1]/(conf_mat[1,2]+conf_mat[1,1])
```

## D)
```{r roc, include=TRUE, message=FALSE, eval=TRUE, tidy=TRUE}
roc.log_reg <- roc(response = test$diabetes, predictor = pred_log_reg, plot=FALSE)
roc.lda <- roc(response = test$diabetes, predictor = pred_lda$posterior[,2], plot=FALSE)
roc.qda <- roc(response = test$diabetes, predictor = pred_qda$posterior[,2], plot=FALSE)
roc.knn <- roc(response = test$diabetes, predictor = prob_knn_adj, plot=FALSE)

plot(roc.log_reg, main="ROC", col="red")
plot(roc.lda, add=TRUE, col="green")
plot(roc.qda, add=TRUE, col="blue")
plot(roc.knn, add=TRUE, col="black")
legend('topright', c("log_reg","lda", "qda", "knn"),
       lty=1, col=c('red', 'green', 'blue',' black'), bty='n', cex=.75)
```

### TODO: Which model performs better?
### TODO: Which model is interpretable and performs well?


# Problem 4

## A)
Show that for the linear regression model $Y=X\beta+\epsilon$ the LOOCV statistic can be computed by the following formula

$CV = \frac{1}{N}\sum^N_{i=1}(\frac{y_i - \hat{y}_i}{1-h_i})^2$, where $h_i=X^T_i(X_TX)^{-1}x_i$, and $x^T_i$ is the ith row of X.

The estimate of $\beta$ is given by
$\hat{\beta_i}=(X^T_{(-i)}X_{(-i)})X_{(-i)}Y$

This gives us that $y_i-\hat{y}_i = y_i - x^T_{(-i)\hat{\beta_i}$

We know that $X^T_{(-i)}X_{(-i)}=(X^TX−x_{i}x^T_i)$
We can then use the Sherman Morrison formula, to find $(X^T_{(-1)}X_{(-1)})^{-1} = (X^TX)^{−1}+\frac{(X^TX)^{−1}x_{i}x^T_{i}(X^TX)^{−1}}{1−h_i}$

We can use this to find $\hat{\beta}$

$$\hat{\beta}_i 
= ((X^TX)^{−1}+\frac{(X^TX)^{−1}x_{i}x^T_{i}(X^TX)^{−1}}{1−h_i})(X^TY-x_iy_i)
= \hat{\beta} - (\frac{(X^TX)^{−1}x_{i}}{1−h_i})(y_i(1-h_i)-x^T_i\hat{\beta}+h_iy_i)
=\hat{\beta}-(X^TX)^{-1}x_i\frac{y_i - \hat{y}_i}{1-h_i}
$$

Therefore we get, 

$$
y_i - \hat{y}_i = y_i-x^T_i\hat{\beta} = y_i-x^T_i(\hat{\beta}-(X^TX)^{-1}x_i\frac{y_i - \hat{y}_i}{1-h_i}) 
= y_i - \hat{y}_i \ h_i \frac{y_i - \hat{y}_i}{1-h_i}
= \frac{y_i - \hat{y}_i}{1-h_i}
$$

## B)

1. False
2. False
3. ?
4. False

# Problem 5

## A) 
```{r ,r_squared include=TRUE, message=FALSE, eval=TRUE, tidy=TRUE}
id <- "19auu8YlUJJJUsZY8JZfsCTWzDm6doE7C" # google file ID
d.bodyfat <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)

rsquared <- function(data, indices) {
  d <- data[indices,] # select samples
  fit <- lm(bodyfat ~ age + weight + bmi, data = d)
  return(summary(fit)$r.square)
}

rsquared(d.bodyfat, nrow(d.bodyfat))
```
The R^2 for a linear regression model is 0.5803

## B)
```{r logistic regression, include=TRUE, message=FALSE, eval=TRUE, tidy=TRUE}
set.seed(4268)

# bootstrapping with 1000 replications
results <- boot(data=d.bodyfat, statistic=rsquared,
                R=1000, formula=bmi_formula)

# view results
results
plot(results)

# get 95% confidence interval
boot.ci(results, type="bca")
```
