---
title: "CompulsoryExercise2"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(error=TRUE)
```

```{r libraries,eval=FALSE,include=FALSE}
#install.packages("knitr") #probably already installed
#install.packages("rmarkdown") #probably already installed
#install.packages("ggplot2") #plotting with ggplot
#install.packages("ggfortify")
#install.packages("leaps")
#install.packages("glmnet")
#install.packages("tree")
#install.packages("caret")
#install.packages("randomForest")
#install.packages("readr")
#install.packages("e1071")
#install.packages("dplyr")
#install.packages("crop")
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(leaps)
library(glmnet)
library(tree)
library(caret)
library(randomForest)
library(readr)
library(e1071)
library(dplyr)
library(crop)
```


# Problem 1


## 1A)

### i) True
### ii) True
### iii) True
### iv) False


## 1B)

```{r load data, include=TRUE}
id <- "1iI6YaqgG0QJW5onZ_GTBsCvpKPExF30G" # google file ID
catdat <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)


set.seed(4268)
train.ind = sample(1:nrow(catdat), 0.5*nrow(catdat))
catdat.train = catdat[train.ind,]
catdat.test = catdat[-train.ind,]
regfit_full <- regsubsets(birds~., data = catdat.train, nvmax=17)
reg_summary <- summary(regfit_full)
```


```{r plot, include=TRUE}


par(mfrow=c(2,2))
min_rss <- which.min(reg_summary$rss)
plot(reg_summary$rss ,xlab="Number of Variables",ylab="RSS",type="l")
points(min_rss,reg_summary$rss[min_rss], col="red",cex=2,pch=20)
plot(reg_summary$adjr2 ,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
max_adj <- which.max(reg_summary$adjr2)
points(max_adj,reg.summary$adjr2[max_adj], col="red",cex=2,pch=20)
plot(reg_summary$cp ,xlab="Number of Variables",ylab="Cp",type="l")
min_cp <- which.min(reg_summary$cp )
points(min_cp,reg_summary$cp [min_cp],col="red",cex=2,pch=20)
min_bic <- which.min(reg.summary$bic)
plot(reg_summary$bic ,xlab="Number of Variables",ylab="BIC",type="l")
points(min_bic,reg_summary$bic [min_bic],col="red",cex=2,pch=20)
```

```{r s, include=TRUE}
plot(regfit.full, scale="bic")
```
As we see from the plot of RSS, Adjusted RSq, Cp and BIC, the metrics for the number of variables are quite the same for RSS, Cp and Adjusted RSq. The plot which the number of variables have the most impact is BIC. The best score of BIC is for 6 variables. To see the variables which are selected, we use the plot function for regsubsets. This shows that the selected variables are wetfood, daily.playtime, children.13, urban, bell and daily.outdoortime. It is also possible to select the variables using cross-validation, as shown below

```{r, include=TRUE}

# Predict function for regsubsets
predict.regsubsets = function(object, newdata, id, ...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form, newdata)
  coefi=coef(object, id=id)
  xvars=names(coefi)
  mat[, xvars]%*%coefi
}


k = 10
set.seed(1)
folds=sample(1:k, nrow(catdat.train), replace = TRUE)
cv.errors = matrix(NA, k, ncol(catdat.train))

# Perform Cross Validation
for (j in 1:k){
  best_subset_method = regsubsets(birds ~ ., data = catdat.train[folds!=j,], nvmax = ncol(catdat.train))
  for (i in 1:(ncol(catdat.train)-1)){
    pred = predict(best_subset_method, catdat.train[folds==j,], id=i)
    cv.errors[j,i] = mean((catdat.train$birds[folds==j]-pred)^2)
  }
}

mean.cv.errors = apply(cv.errors,2,mean)
mean.cv.errors
```
Using cross validation, it is the model with 13 variables that are the best. To calculate the mse, we use the model with 6 variables from the first results, instead of cross-validation. 

```{r, include=TRUE}
number_of_variables <- 6

variables <- names(coef(best_subset_method, id=number_of_variables))
variables <- variables[!variables %in% "(Intercept)"]
bsm_formula <- as.formula(best_subset_method$call[[2]])
bsm_design_matrix <- model.matrix(bsm_formula, catdat.train)[, variables]
bsm_data_train <- data.frame(birds = catdat.train$birds, bsm_design_matrix)

best_subset_model <- lm(formula = bsm_formula, bsm_data_train)

# Prediction on test set
test_matrix <- model.matrix(bsm_formula, catdat.test)[, variables]
predictions <- predict(object= best_subset_model, newdata = as.data.frame(test_matrix))

# MSE Error
mse.best_subset <- mean((catdat.test$birds - predictions)^2)
cat("The mse of the best subset model are", mse.best_subset)
```

### C)

```{r k, include=TRUE}
x.train <- model.matrix(birds~., data = catdat.train)[,-1]
y.train <- catdat.train$birds
x.test = model.matrix(birds~., data = catdat.test)[,-1]
y.test = catdat.test$birds
```

```{r, include=TRUE}
set.seed(1)
grid=10^seq(10,-2,length=100)

# Fitting a lasso model 
lasso.mod = glmnet(x.train, y.train, alpha=1, lambda = grid)

# Cross validation on the model to find the correct lambda 
cv.out=cv.glmnet(x.train,y.train,alpha=1)

# Choose the lamda which gives the lowest score on the cross-validation
best_lambda=cv.out$lambda.min

#Predict on the test set
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x.test)

# Calculate the MSE from the predictions and the actual values in the test set
mse.lasso = mean((lasso.pred-y.test)^2)
cat("The MSE for the lasso regression model are ", mse.lasso, "\n")

# Find the non zero coefficients
out=glmnet(x.test,y.test,alpha=1,lambda=grid)
lasso.coefs <- coef(out, s=best_lambda)
non_zero.coef = lasso.coefs[which(lasso.coef!=0)]
coef.names = coefs@Dimnames[[1]][which(lasso.coef!=0)]
results <- data.frame(
  features = coef.names, #intercept included
  coefs    = non_zero.coef  #intercept included
)

# The non zero coefficients are:
results
```

### D)
The tuning parameter serves the purpose of controlling the RSS and the shrinkage penalty, $\lambda \sum \beta^2_j$. When $\lambda \rightarrow \infty$, the ridge regression coefficients estimates will approach zero, since the impact the of the shrinkage penalty increases. When the tuning parameter is zero, ridge regression is the same as RSS, since the shrinkage penalty has no effect.

### E)

```{r, include=TRUE}
# Model with only intercept
model.intercept <- lm(birds ~ 1, catdat.train)

# Model with all covariates 
model.all <- lm(birds ~ ., catdat.train)

prediction.intercept <- predict(model.intercept, catdat.test)
prediction.all <- predict(model.all, catdat.test)

mse.intercept <- mean((catdat.test$birds - prediction.intercept)^2)
mse.all <- mean((catdat.test$birds - prediction.all)^2)

cat("The mse of only intercept model are ", mse.intercept, "\n")
cat("The mse of all covariates model are ", mse.all)
```

### F)
```{r, include=TRUE}
results <- matrix(c(mse.best_subset, mse.lasso, mse.intercept, mse.all), ncol=1)
rownames(results) <- c("Best subset model", "Lasso", "Model with only intercept", "Multiple linear regression using all covariates")
colnames(results) <- c("MSE")
results
```
From the table, we can see that the best model is the lasso model. In second place we have the best subset model. We can also see that the linear regression model with all covariates is almost as good as the subset model, and the only intercept model is way worse. It does fit what is expected, as we could see from the metric plots from B) that the model with all covariates was almost as good as the subset model.


# Problem 2.

## 2A) 
 
1. True
2. False 
3. False 
4. False

## 2B)

One way of representing a cubic spline is to use $x, x^2, x^3$ and then add one trucated power basis function per knot. 

The truncated power basis function is defined as $h(x,\xi) = (x-\xi)^3_+$ if x is greater than $\xi$ and 0 otherwise.  

This gives us the basis functions $x, x^2, x^3, (x-q_1)^3, (x-q_2)^3$

## 2C)

```{r, include=TRUE}
ds = catdat.train[c("birds", "daily.outdoortime")]
n = nrow(ds)

deg = 1:10
set.seed(1)


# Colors to use for lines
co = rainbow(length(deg))


dat = c()

for (i in deg) {
  model = lm(birds ~ poly(daily.outdoortime, i), data=ds)
  poly.predictions <- model %>% predict(ds)
  dat = rbind(
    dat,
    data.frame(
      daily.outdoortime = ds$daily.outdoortime, 
      birds = model$fit, 
      degree=as.factor(rep(i, length(model$fit)))
    )
  )
  mse.poly[i] = mean((poly.predictions - ds$birds)^2)
}

ggplot(data = ds, aes(x=daily.outdoortime, y=birds)) + geom_point(color="darkgrey") + labs(title = "Polynomial regression") + geom_line(data=dat,  aes(x=daily.outdoortime, y=birds, color=degree))

# Training MSE fro all polynomial regression models 
MSEdata = data.frame(MSE=mse.poly, degree=deg)
ggplot(data=MSEdata, aes(x=degree, y=MSE)) + geom_line() + geom_point() + labs(title = "MSE error")
```
Polynomial regression is a regression form, where there are added more predictors, which are the original predictors raised to a power. Instead of using the regular for a linear model $y_i =\beta_0 + \beta_1 x_1 + \xi_i$, it is used a polynomial function, $y_1 = \beta_0 + \beta_1 x_1 + \beta_2x_1^2+ ... + \beta_d x_i^d + \xi_i$.  



# Problem 3.

## 4A)

### i) True
### ii) True
### iii) False
### iv) False

## 4B)
By looking at the tree above, we can conclude that we will have two nodes left, which are age < 81.5, and age < 46.5. This will give us three leaves, two for the split between age < 46.5, and one for the split between age < 81.5. The reason we are left with age < 46.5 and age < 81.5 as nodes, are that they were the first nodes which created.   

## 3C)

### i)

```{r}
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E" # google file ID
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
d.train=d.diabetes$ctrain
d.test=d.diabetes$ctest
```

```{r, include=TRUE}
set.seed(1)
d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)
t.diabetes = tree(diabetes ~ ., data=d.train)
summary(t.diabetes)

cv.diabetes=cv.tree(t.diabetes, FUN=prune.misclass)
min_idx = which.min(cv.diabetes$dev)
plot(cv.diabetes$size,cv.diabetes$dev,type="b",
                   xlab="Terminal nodes")

cv.diabetes$size[min_idx]
prune.diabetes <- prune.misclass(t.diabetes, best=17)
predictions = predict(prune.diabetes, d.test, type = "class")
# test confusion
pred_matrix <- table(predicted = predictions , actual = d.test$diabetes)
mis_error <- (pred_matrix[1,2] + pred_matrix[2,1]) /sum(pred_matrix)
cat("The missclassification error is ", mis_error)
```

### ii)
```{r, include=TRUE}
set.seed(1)
rft.diabetes <- randomForest(diabetes ~ ., data=d.train, importance=TRUE)
rft.diabetes

predictions = predict(rft.diabetes, d.test, type = "class")
pred_matrix <- table(predicted = predictions , actual = d.test$diabetes)
mis_error <- (pred_matrix[1,2] + pred_matrix[2,1]) /sum(pred_matrix)
cat("The missclassification error is ", mis_error, "\n")

rft.importance <- importance(rft.diabetes)
rft.importance
```
We can see from the statistics of mean decrease in accuracy and gini for the different variables, that the most influential variables are bmi and glu.


# Problem 4
## 4A)
### i) False
### ii) True
### iii) False
### iv) True

## 4B)

```{r task4b_data}
library(e1071)

id <- "1x_E8xnmz9CMHh_tMwIsWP94czPa1Fpsj"  # google file ID
d.leukemia <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id), header = T)

set.seed(2399)
t.samples <- sample(1:60, 15, replace = F)
d.leukemia$Category <- as.factor(d.leukemia$Category)
d.leukemia.test <- d.leukemia[t.samples, ]
d.leukemia.train <- d.leukemia[-t.samples, ]
```

### i)
SVM is a more suitable method here than logistic regression as logistic regression usually
needs about ten times more cases than the number of explanatory variables. SVM on the other hand works
well in high dimensional spaces. To achieve a good performance one could instead use random forest.
### ii)
The paper proposes an ensamble SVM-Recursive Feature Elimination. They use this method for gene selection and classification.

### iii)
```{r task4biii}
svmfit=svm(Category~., data=d.leukemia.train, kernel="linear", cost=1, scale=TRUE)
summary(svmfit)

pred.train<-predict(svmfit, d.leukemia.train)
pred.test<-predict(svmfit, d.leukemia.test)

table(pred.train, d.leukemia.train$Category)
sum(pred.train!=d.leukemia.train$Category)/length(pred.train)
table(pred.test, d.leukemia.test$Category)
sum(pred.test!=d.leukemia.test$Category)/length(pred.test)
```
From the above code-snippet we see that the error rate for the training set was 0% and
for the test set it was 26.67%. 
No, it is not surprising to see that the misclassification rate on the training set
is 0%, as there are so many features compared to the number of and the data is thus
probably linearly separable.
The most common type of error that we see in the test set are type 2 errors.
The sensitivity in this case is 0.333, which is very low and the classification
method is not successful.

#The most common error type in the test set is type 2 errors: there are 4 persons that
#are classified as Non-Relapse, that relapsed.
#No, the classification method is not successful as it has a sensitivity of 0.2, which is
#not good.

### iv)

```{r task4biv_0.00001}
svmfit_radial=svm(Category~., data=d.leukemia.train, kernel="radial", cost=1, scale=TRUE, gamma=0.00001, na.action=na.omit)
pred.train<-predict(svmfit_radial, d.leukemia.train)
table(pred.train, d.leukemia.train$Category)
sum(pred.train!=d.leukemia.train$Category)/length(pred.train)
pred.test<-predict(svmfit_radial, d.leukemia.test)
table(pred.test, d.leukemia.test$Category)
sum(pred.test!=d.leukemia.test$Category)/length(pred.test)
summary(svmfit_radial)
```

```{r task4biv_0.01}
svmfit_radial=svm(Category~., data=d.leukemia.train, kernel="radial", cost=1, scale=TRUE, gamma=0.01, na.action=na.omit)
pred.train<-predict(svmfit_radial, d.leukemia.train)
table(pred.train, d.leukemia.train$Category)
sum(pred.train!=d.leukemia.train$Category)/length(pred.train)
pred.test<-predict(svmfit_radial, d.leukemia.test)
table(pred.test, d.leukemia.test$Category)
sum(pred.test!=d.leukemia.test$Category)/length(pred.test)
summary(svmfit_radial)
```
From the plots above, we see that when $\gamma$ is 1e-5, it predicts that everything in the training- and test set is
Non-Relapse, while when $\gamma$ is 1e-2, it fits the training set prefectly, but predicts that everything in the test
set is Non-Relapse. The reason for this is that a large value of $\gamma$ introduces more variance, and small values of
$\gamma$ gives more bias. In the first example above, the predictions are biased towards the majority of training targets, Non-Relapse. And as explained, this is very natural. For both values of $\gamma$ for the radial svm, we achieved a misclassification rate of 0.4 on the test set, which is worse than in iii. Considering the training set, a $\gamma$ of 1e-2 gives the same result (prefect fit) as in iii, while a $\gamma$ of 1e-5 gives a misclassification rate of 1/3.
## 4C)

$$K(x_i, x_i')=(1+\sum_{j=1}^{p}{x_{ij}x_{i'j'}})^d$$
Writing this out with $d=2$ gives:
$$K(x_i, x_i')=1+(\sum_{j=1}^{p}{x_{ij}x_{i'j'}})(\sum_{j=1}^{p}{x_{ij}x_{i'j'}})+2*\sum_{j=1}^{p}{x_{ij}x_{i'j'}}$$

Now, if we rewrite this into sums of factors, where is factor is either from $x_i$ or $x_i'$ we get:
$$K(x_i, x_i')=\sum_{j=1}^{p}{(x_{ij}^2)(x_{i'j'}^2)}+\sum_{j=2}^{p}{\sum_{k=1}^{j-1}{(\sqrt{2}x_{ij}x_{i'k'})(\sqrt{2}x_{ij}x_{i'k'}})}+\sum_{k=1}^{p}{(\sqrt{2}x_{ij})(\sqrt{2}x_{i'j'}})+1$$
Then we can easily see that if

$$h(X)=(h_1(X)=X_1^2, h_2(X)=X_2^2, h_3(X)=\sqrt{2}X_1, h_4(X)=\sqrt{2}X_1, h_5(X)=\sqrt{2}X_2, h_6(X)=1)$$
then
$$K(X, X')=\langle h(X), h(X')\rangle=\sum_{j=1}^{p}{(x_{ij}^2)(x_{i'j'}^2)}+\sum_{j=2}^{p}{\sum_{k=1}^{j-1}{(\sqrt{2}x_{ij}x_{i'k'})(\sqrt{2}x_{ij}x_{i'k'}})}+\sum_{k=1}^{p}{(\sqrt{2}x_{ij})(\sqrt{2}x_{i'j'}})+1=(1+\sum_{j=1}^{p}{x_{ij}x_{i'j'}})^2$$

# Problem 5
## 5A)
### i)
True
### ii)
False
### iii)
False
### iv)
False

## 5B)
```{r task5_handout}
x1 <- c(1, 2, 0, 4, 5, 6)
x2 <- c(5, 4, 3, 1, 1, 2)
```

### i)
```{r task5bi}
set.seed(1)
clusters<-sample(c(1,2), size=length(x1), replace=TRUE)
clusters

plot(x1, x2, col=clusters, pch=19, main="clustering")
legend("topleft", legend=c("1", "2"), col=c("black", "red"), bty="o", cex=0.8, pch=19)
```

## ii)
```{r task5bii}
center.1<-c(
  sum(x1[clusters==1])/length(x1[clusters==1]),
  sum(x2[clusters==1])/length(x2[clusters==1]))
center.1
center.2<-c(
  sum(x1[clusters==2])/length(x1[clusters==2]),
  sum(x2[clusters==2])/length(x2[clusters==2]))
center.2

plot(x1, x2, col=clusters, pch=19, main="clustering")
points(center.1[1], center.1[2], col="gray", pch=19)
points(center.2[1], center.2[2], col="darkred", pch=19)
legend("bottomleft", legend=c("1", "2", "center 1", "center 2"), col=c("black", "red", "gray", "darkred"), bty="o", cex=0.8, pch=19)
```


## iii)
```{r task5biii}
euc_dist<-function(a, b) sqrt(sum((a-b)^2))
points<-matrix(c(x1, x2), nrow=length(x1), ncol=2, byrow=FALSE)
clusters<-apply(points, 1, function(point) ifelse(euc_dist(point, center.1) < euc_dist(point, center.2),1, 2))

plot(x1, x2, col=clusters, pch=19, main="clustering")
legend("topleft", legend=c("1", "2"), col=c("black", "red"), bty="o", cex=0.8, pch=19)
```


# Task 5c

```{r task5chandout}
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU"  # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", 
                             id), header = F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "")
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "")
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
GeneData = t(GeneData)
GeneData <- scale(GeneData)
```

```{r task5c}
par(mfrow=c(2,3))
euc_dists<-dist(GeneData)
plot(hclust(euc_dists, method="complete"), main="Complete linkage with euclidian distance")
plot(hclust(euc_dists, method="single"), main="Single linkage with euclidian distance")
plot(hclust(euc_dists, method="average"), main="Average linkage with euclidian distance")
corr_dists<-as.dist(1-cor(t(GeneData)))
plot(hclust(corr_dists, method="complete"), main="Complete linkage with correlation-based distance")
plot(hclust(corr_dists, method="single"), main="Single linkage with correlation-based distance")
plot(hclust(corr_dists, method="average"), main="Average linkage with correlation-based distance")
```


# Task 5d

```{r task5d}
target=c(rep(1, 20), rep(2, 20))
ct = cutree(hclust(euc_dists, method="complete"), k=2)
sum(ct!=target)/length(target)
ct = cutree(hclust(euc_dists, method="single"), k=2)
sum(ct!=target)/length(target)
ct = cutree(hclust(euc_dists, method="average"), k=2)
sum(ct!=target)/length(target)
ct = cutree(hclust(corr_dists, method="complete"), k=2)
sum(ct!=target)/length(target)
ct = cutree(hclust(corr_dists, method="single"), k=2)
sum(ct!=target)/length(target)
ct = cutree(hclust(corr_dists, method="average"), k=2)
sum(ct!=target)/length(target)
```

We see that all the trees classifies equally well.

# Task 5e

## i)
```{r task5ei}
GeneData.pca<-prcomp(GeneData, center=TRUE, scale=TRUE)
GeneData.pca$x[,1]
GeneData.pca$x[,2]

plot(GeneData.pca$x[,1], GeneData.pca$x[,2], col=c(rep(1, 20), rep(2, 20)))
```

## ii)
```{r task5eii}
summary(GeneData.pca)
```
The 5 first principal components explains 21.097% of the total variance.

# Task 5f

```{r task5f}
rot_1 = GeneData.pca$rotation[,1]
rot_1[order(abs(rot_1), decreasing = TRUE)][1:50]
```

